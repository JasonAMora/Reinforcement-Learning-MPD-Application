{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application to Linear Programming: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Reinforcement Learning?\n",
    "\n",
    "Reinforcement learning, in Layman's, is the science of decision making. The goal of reinforcement learning is to use the reward signal as a marker of success or failure and use that to guide the agent through the various situations where it will maximize the best decision with respect to each state and action.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Lets take the journey understand the world of decision making and see how it relates to Math 680 (Optimization)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chains and Markov Decision Processes (MDP)\n",
    "\n",
    "### Markov Chains\n",
    "\n",
    "Defintion: A subset $\\mathcal{X}$ is a bounded compact Euclidean space, the discrete-time dynamic system $x_{t}$, where $t \\in \\mathbb{N}$ is a Markov Chain if\n",
    "\n",
    "#### $$ P(x_{t+1} = x| x_{t},x_{t-1},...,x_{0}) \\ = \\ P(x_{t+1} = x|x_{t}) $$\n",
    "\n",
    "\n",
    "so that all of the information needed to predict the future is contained in the current state. This is also known as the $Markov \\ Property$\n",
    "\n",
    "<br>\n",
    "\n",
    "NOTE: $P(x_{t+1} = x|x_{t})$ is the transition probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decisions Process\n",
    "Defintion: $\\mathcal{M} \\ = (S,A,p,r)$\n",
    "\n",
    "- $S$ = {$s_0$, $s_1$, $s_2$, ..., $s_n$}, is a finite set of states\n",
    "- $A$ = {$a_0$, $a_1$, $a_2$,..., $a_n$}, is a finite set of actions\n",
    "- $p$ = $P(s_{t+1}|S = s_{t}, A = a_{t})$, is the $transition$ $probability$ to the next state, given state $s$ and action $a$\n",
    "- $r(s_{t},a,s_{t+1})$ is the $reinforcement$ obtained when taking action $a$, a transition from a state $s$ to a state $s_{t+1}$ is observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy\n",
    "Defintion: At a time $t \\in \\mathbb{N}$, a $decision \\ rule \\ \\pi_{t}$, is a mapping from state to actions. In particular:\n",
    "\n",
    "- Deterministic $\\pi_{t}:S \\rightarrow A$, where $\\pi_{t}(s)$ denotes the action chosen at state $s$ at time $t$\n",
    "- Stochastic $\\pi_{t}:S \\rightarrow \\Delta(A)$, where $\\pi_{t}(a|s)$ denotes the probability of taking an action $a$ at state $s$ at time $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a Markov Decision Process\n",
    "\n",
    "\n",
    "<img src = \"MPD Example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Value (State Value Function)\n",
    "\n",
    "We can define the utility value or State value function as the following with a given policy $\\pi$ \n",
    "\n",
    "$$U^{\\pi}([s_0,s_1,s_2, ... ]) =  \\sum_{t = 0}^{\\infty}{\\gamma^{t} \\ r(s_{t},\\pi(s_{t}))}$$\n",
    "\n",
    "$where \\  0 \\leq \\gamma < 1$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### We can take this equation\n",
    "\n",
    "$$U^{\\pi}(s)= {E \\ [} \\sum_{t = 0}^{\\infty}{\\gamma^{t} \\ r(s_{t},\\pi(s_{t}))|s_{t} = s, \\pi(s_t)] }$$\n",
    "\n",
    "\n",
    "\n",
    "### and derive this\n",
    "\n",
    "\n",
    "$$U^{\\pi}(s) = r\\ (s, a) + \\gamma {\\sum_{s' \\in \\ S}}P(s'| s, a) \\ U^{\\pi}(s')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is this all related to Linear Optimization?!\n",
    "\n",
    "We are going to start with some motivation and stumble ourselves to a linear program model.\n",
    "\n",
    "#### Lets look at the Bellman Equation\n",
    "\n",
    "$$U^{\\pi}(s) = r\\ (s, a) + \\gamma {\\sum_{s' \\in \\ S}}P(s'|s, a) \\ U^{\\pi}(s')$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Lets make some adjustments to this and write this in a more compact and familiar way to look at it and approximate it linearly\n",
    "\n",
    "$U^{\\pi_k} = r + \\gamma P \\ U^{\\pi_k}$\n",
    "$\\implies U^{\\pi_k} - \\gamma P \\ U^{\\pi_k} = r$\n",
    "$\\implies (I - \\gamma P) \\ U^{\\pi_k} = r$\n",
    "\n",
    "$\\therefore$\n",
    "\n",
    "\n",
    "$U^{\\pi_k} = (I - \\gamma P)^{-1} r$\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "The BIG question: Why is $(I - \\gamma P)$ invertible? \n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "Proof:\n",
    "\n",
    "$P$ is a stochastic matrix, and $0 \\leq \\gamma < 1$\n",
    "\n",
    "$\\implies ||I||_{\\infty} - \\gamma|| P||_{\\infty}  \\leq ||I - \\gamma P||_{\\infty} $\n",
    "\n",
    "<br>\n",
    "\n",
    "Using the Upper Bound Theorem: $\\rho(A) \\leq ||A||$\n",
    "\n",
    "\n",
    "$\\implies \\rho(I) - \\gamma \\ \\rho(P) \\leq ||I - \\gamma P||_{\\infty} $\n",
    "\n",
    "$\\implies 1 - \\gamma \\leq ||I - \\gamma P||_{\\infty} $\n",
    "\n",
    "$\\implies 0 < 1 - \\gamma, \\ for \\ 0 \\leq \\gamma < 1$\n",
    "\n",
    "$\\implies$ the eigenvalues are never zero.\n",
    "\n",
    "$\\therefore$\n",
    "\n",
    "$(I - \\gamma P)$ is invertible\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "Moving along! \n",
    "<br>\n",
    "Consider the next iteration in the sequence,\n",
    "<br>\n",
    "\n",
    "$U^{\\pi_{k + 1}} = (I - \\gamma P)^{-1} r$\n",
    "\n",
    "We are going to perform our favorite trick, add zero!\n",
    "\n",
    "$\\implies U^{\\pi_{k + 1}} = (I - \\gamma P)^{-1} r + U^{\\pi_k} - U^{\\pi_k}$\n",
    "\n",
    "\n",
    "$U^{\\pi + 1} = U^{\\pi_k} + (I - \\gamma P)^{-1} r - U^{\\pi}$\n",
    "\n",
    "$U^{\\pi + 1} = U^{\\pi_k} + [(I - \\gamma P)^{-1}r - \\ U^{\\pi_k}]$\n",
    "\n",
    "$U^{\\pi + 1} = U^{\\pi_k} + (I - \\gamma P)^{-1} [r - (I - \\gamma P) \\ U^{\\pi_k}]$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "We will defined $T = (I - \\gamma P)$\n",
    "\n",
    "$\\therefore$\n",
    "\n",
    "$U^{\\pi_{k + 1}} = U^{\\pi_k} + T^{-1} [r - T \\ U^{\\pi_k}]$\n",
    "\n",
    "We have a linear equation where $U^{\\pi_k}$ are the independent variables, for $k \\in \\mathbb{N}$\n",
    "\n",
    "\n",
    "#### This is a hyperplane! Our best friend!\n",
    "\n",
    "#### Why?\n",
    "<br>\n",
    "Since we have proved earlier in the semester that all hyperplanes are convex, we are guaranteed to find a solution to a linear program. Therefore, for some fixed $k$, we can solve the linear program $U^{\\pi_{k + 1}}$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "We can express our linear program as follows:\n",
    "\n",
    "\n",
    "### $$Primal \\ Linear \\ Program$$\n",
    "\n",
    "$$\\min{\\sum_{s' \\in S}{U(s)}}$$\n",
    "\n",
    "$$subject \\ to$$\n",
    "\n",
    "$$R(s,a) + \\gamma \\sum_{s' \\in S}{P(s'|s,a)U(s')} \\leq U(s)$$\n",
    "\n",
    "$$\\forall s \\in S \\ and \\ \\forall a \\in A$$\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "We can also express the dual\n",
    "\n",
    "\n",
    "### $$Dual \\ Linear \\ Program$$\n",
    "\n",
    "$$\\max{ \\sum_{s \\in S}\\sum_{a \\in A}{V(s,a)R(s,a)} }$$\n",
    "\n",
    "$$subject \\ to$$\n",
    "\n",
    "$$\\sum_{a' \\in A}{V(s',a')} = b(s') + \\gamma \\sum_{s' \\in S}\\sum_{a' \\in A}{P(s'|s,a)V(s,a)}$$\n",
    "\n",
    "$$\\forall s \\in S \\ and \\ \\forall a \\in A$$\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Using the complementary slackness theorem, know that\n",
    "\n",
    "$$\\min{\\sum_{s' \\in S}{U(s)}} = \\max{ \\sum_{s \\in S}\\sum_{a \\in A}{V(s,a)R(s,a)} }$$\n",
    "\n",
    "<br>\n",
    "\n",
    "### Punchline: Our optimal utility value is the same as the value for our optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# You can relax, time for a demo!\n",
    "\n",
    "\n",
    "# Gridworld Example\n",
    "\n",
    "<img src=\"Gridworld2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calls the numpy library\n",
    "import numpy as np\n",
    "\n",
    "## Calls the library linear programming solver\n",
    "from scipy.optimize import linprog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating a function that will generate the transition problabilites\n",
    "with respect to the agent at a certain location of the map\n",
    "\n",
    "@param:\n",
    "    - Current row (int)\n",
    "    - Current col (int)\n",
    "    - Current action (string)\n",
    "    - Row size of the gridworld (int)\n",
    "    - Column size of the gridworld (int)\n",
    "    - Wall cells (list of tuples)\n",
    "    - Green cells (list of tuples)\n",
    "    - Red cells (list of tuples)\n",
    "    \n",
    "@return matrix (numpy matrix)\n",
    "\"\"\"\n",
    "\n",
    "def return_transition(row, col, action, tot_row, tot_col, wall_cells, green_cells, red_cells):\n",
    "    \n",
    "    ## Checks to see if the current row and column is out of the gridworld\n",
    "    if(row > tot_row - 1 or col > tot_col - 1):\n",
    "        print(\"ERROR: the index is out of range...\")\n",
    "        return None\n",
    "    \n",
    "    ## Extends the gridworld\n",
    "    extended_world = np.zeros((tot_row + 2, tot_col + 2))\n",
    "    \n",
    "    ## Checks to see if the current row and column is a 'wall' cell\n",
    "    for tupes in wall_cells:\n",
    "        if (row, col) == tupes:\n",
    "            return extended_world[1:tot_row + 1, 1:tot_col + 1]\n",
    "        \n",
    "    ## Checks to see if the current row and column is a 'green' cell\n",
    "    for tupes in green_cells:\n",
    "        if (row, col) == tupes:\n",
    "            return extended_world[1:tot_row + 1, 1:tot_col + 1]\n",
    "        \n",
    "    ## Checks to see if the current row and column is a 'red' cell\n",
    "    for tupes in red_cells:\n",
    "        if (row, col) == tupes:\n",
    "            return extended_world[1:tot_row + 1, 1:tot_col + 1]\n",
    "        \n",
    "    ## Fills in the probabilities according with respect to the direction\n",
    "    if(action == \"North\"):\n",
    "            col += 1\n",
    "            row += 1\n",
    "            extended_world[row - 1, col] = 0.8\n",
    "            extended_world[row, col + 1] = 0.1  \n",
    "            extended_world[row, col - 1] = 0.1 \n",
    "    \n",
    "    \n",
    "    elif(action == \"South\"): \n",
    "            col += 1\n",
    "            row += 1\n",
    "            extended_world[row + 1, col] = 0.8\n",
    "            extended_world[row, col + 1] = 0.1  \n",
    "            extended_world[row, col - 1] = 0.1\n",
    "    \n",
    "    \n",
    "    elif(action == \"West\"):\n",
    "            col += 1\n",
    "            row += 1\n",
    "            extended_world[row - 1, col] = 0.1\n",
    "            extended_world[row + 1, col] = 0.1  \n",
    "            extended_world[row, col - 1] = 0.8\n",
    "    \n",
    "    \n",
    "    elif(action == \"East\"):\n",
    "            col += 1\n",
    "            row += 1\n",
    "            extended_world[row - 1, col] = 0.1\n",
    "            extended_world[row + 1, col] = 0.1  \n",
    "            extended_world[row, col + 1] = 0.8\n",
    "\n",
    "    ## Resets the walls\n",
    "    for tupes in (np.array(wall_cells) + 1):\n",
    "        \n",
    "        if extended_world[tupes[0], tupes[1]] != 0:\n",
    "            extended_world[row, col] += extended_world[tupes[0], tupes[1]]\n",
    "            \n",
    "        extended_world[tupes[0], tupes[1]] = 0.0\n",
    "    \n",
    "    \n",
    "    ## Gridworlds that have smaller rows than columns \n",
    "    if tot_row < tot_col:\n",
    "    \n",
    "        ## Checks if the robot is bouncing along the walls per row\n",
    "        for r in range(0, tot_row + 2):\n",
    "\n",
    "            if(extended_world[r, 0] != 0):\n",
    "                extended_world[r, 1] += extended_world[r, 0]\n",
    "\n",
    "\n",
    "            if(extended_world[r, tot_row + 2] != 0): \n",
    "                extended_world[r, tot_row + 1] += extended_world[r, tot_row + 2]\n",
    "\n",
    "\n",
    "        ## Checks if the robot is bouncing along the walls per columns\n",
    "        for c in range(0, tot_col + 2):\n",
    "\n",
    "            if(extended_world[0, c] != 0):\n",
    "\n",
    "                extended_world[1, c] += extended_world[0, c]\n",
    "\n",
    "            if(extended_world[tot_col, c] != 0): \n",
    "                extended_world[tot_col - 1, c] += extended_world[tot_col, c]\n",
    "\n",
    "        return extended_world[1:tot_row + 1, 1:tot_col + 1] \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Grid worlds where the dimensions are the same\n",
    "    if tot_row == tot_col:\n",
    "\n",
    "        ## Checks if the robot is bouncing along the walls per row\n",
    "        for r in range(0, tot_row + 2):\n",
    "\n",
    "            if(extended_world[r, 0] != 0):\n",
    "                extended_world[r, 1] += extended_world[r, 0]\n",
    "\n",
    "\n",
    "            if(extended_world[r, tot_row + 1] != 0): \n",
    "                extended_world[r, tot_row] += extended_world[r, tot_row + 1]\n",
    "\n",
    "\n",
    "        ## Checks if the robot is bouncing along the walls per columns\n",
    "        for c in range(0, tot_col + 2):\n",
    "\n",
    "            if(extended_world[0, c] != 0):\n",
    "\n",
    "                extended_world[1, c] += extended_world[0, c]\n",
    "\n",
    "            if(extended_world[tot_col, c] != 0): \n",
    "                extended_world[tot_col - 1, c] += extended_world[tot_col, c]\n",
    "\n",
    "        return extended_world[1:tot_row + 1, 1:tot_col + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a function that will create the rewards vector\n",
    "## @param: dimension of the gridworld; rows (int), columns (int)\n",
    "## @param (cont'd) reward value (float), green cell value (float), red cell value (float)\n",
    "## @param (cont'd: green locations (list of tuples), list of red locations (list of tuples)\n",
    "## @param (cont'd) locations of walls (list of tuples)\n",
    "## @return: vector of the reward structure\n",
    "def rewards_vector(tot_row, tot_col\n",
    "                   ,reward, green_value, red_value\n",
    "                  ,wall_cells, green_cells, red_cells):\n",
    "    \n",
    "    ## Creates a blank grid\n",
    "    r = np.zeros((tot_row, tot_col))\n",
    "    \n",
    "    \n",
    "    ## Adds the values for the green cells\n",
    "    for green in green_cells:\n",
    "        r[green] = green_value\n",
    "        \n",
    "        \n",
    "    ## Adds the values for the red cells\n",
    "    for red in red_cells:\n",
    "        r[red] = red_value\n",
    "        \n",
    "        \n",
    "    ## Iterates through the entire grid and puts the reward \n",
    "    ## for all blank cells (i.e. not red, green, or a wall)\n",
    "    for cells in list(np.ndindex(tot_row,tot_col)):\n",
    "        if cells not in green_cells + red_cells + black_cells:\n",
    "            r[cells] = -reward\n",
    "            \n",
    "    \n",
    "    ## returns the reward vector\n",
    "    return r.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a fucntion that will create the Transition Matrix for the MDP\n",
    "def create_MDP_Trans_Matrix(tot_row, tot_col):\n",
    "\n",
    "\n",
    "    T = np.zeros((tot_row * tot_col, tot_row * tot_col, 4))\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for row in range(0, tot_row):\n",
    "\n",
    "        for col in range(0, tot_col):\n",
    "\n",
    "            line = return_transition(row, col, 'North', tot_row, tot_col, black_cells, green_cells, red_cells)\n",
    "            T[counter, : , 0] = line.flatten()\n",
    "\n",
    "            line = return_transition(row, col, 'West', tot_row, tot_col, black_cells, green_cells, red_cells)\n",
    "            T[counter, : , 1] = line.flatten()\n",
    "\n",
    "            line = return_transition(row, col, 'South', tot_row, tot_col, black_cells, green_cells, red_cells)\n",
    "            T[counter, : , 2] = line.flatten()\n",
    "\n",
    "            line = return_transition(row, col, 'East', tot_row, tot_col, black_cells, green_cells, red_cells)\n",
    "            T[counter, : , 3] = line.flatten()\n",
    "\n",
    "            counter += 1\n",
    "            \n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a function that will generate a random policy\n",
    "## @param: grid size world; total # of rows (int), total # of columns (int)\n",
    "## @param (cont'd): list of wall locations (list of tuples), list of terminal states (list of tuples)\n",
    "## @return: policy vector (numpy array)\n",
    "\n",
    "## NOTE ##\n",
    "# NaN = Nothing\n",
    "## -1 = Terminal\n",
    "## 0 = Up\n",
    "## 1 = Left\n",
    "## 2 = Down\n",
    "## 3 = Right\n",
    "\n",
    "def gen_policy(tot_row, tot_col, wall_cells, green_cells, red_cells):\n",
    "    \n",
    "    ## Creates a random policy matrix\n",
    "    p = np.random.randint(0, 4, size = (tot_row , tot_col)).astype(np.float32)\n",
    "    \n",
    "    ## Inputs a NaN value for every wall \n",
    "    for cells in wall_cells:\n",
    "        p[cells] = np.NaN\n",
    "        \n",
    "    ## Inputs a value of -1.0 for every terminal state\n",
    "    for cells in green_cells + red_cells:\n",
    "        p[cells] = -1.0\n",
    "    \n",
    "    return p.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$U^{\\pi}(s) = r\\ (s, a) + \\gamma {\\sum_{s' \\in \\ S}}P(s'| s, a) \\ U^{\\pi}(s')$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Return the utility value\n",
    "\n",
    "@param p policy vector\n",
    "@param u utility vector\n",
    "@param r reward vector\n",
    "@param T transition matrix\n",
    "@param gamma discount factor\n",
    "@return the utility vector u\n",
    "\"\"\"\n",
    "def evaluate_utility_value(p, u, r, T, gamma):\n",
    "    for s in range(len(r)):\n",
    "        if not np.isnan(p[s]):\n",
    "            v = np.zeros((1,len(r)))\n",
    "            v[0,s] = 1.0\n",
    "            action = int(p[s])\n",
    "            u[s] = r[s] + gamma * np.sum(np.multiply(u, np.dot(v, T[:,:,action])))\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$Primal \\ Linear \\ Program$$\n",
    "\n",
    "$$\\min{\\sum_{s' \\in S}{U(s)}}$$\n",
    "\n",
    "$$subject \\ to$$\n",
    "\n",
    "$$R(s,a) + \\gamma \\sum_{s' \\in S}{P(s'|s,a)U(s')} \\leq U(s)$$\n",
    "\n",
    "$$\\forall s \\in S \\ and \\ \\forall a \\in A$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@param p policy vector\n",
    "@param u utility vector\n",
    "@param r reward vector\n",
    "@param T transition matrix\n",
    "@param gamma discount factor\n",
    "@return the utility vector u\n",
    "\"\"\"\n",
    "def evaluate_utility_value_LP(p, u, r, T, gamma):\n",
    "    for s in range(len(r)):\n",
    "        if not np.isnan(p[s]):\n",
    "            v = np.zeros((1,len(r)))\n",
    "            v[0,s] = 1.0\n",
    "            action = int(p[s])\n",
    "            u[s] = linprog(np.ones(len(r)).T, A_ub = - (np.eye(len(r)) -  gamma * T[:,:,action])\n",
    "                           , b_ub = -r, method = 'simplex')['x'][s]\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$\\pi_{k}(s) = \\arg\\max_{a \\in A} \\{r\\ (s, a) + \\gamma {\\sum_{s' \\in \\ S}P(s_t = s'| s_t = s, a) \\ U^{\\pi}(s') \\} }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_expected_action(u, T, v):\n",
    "    \"\"\"Return the expected action.\n",
    "\n",
    "    It returns an action based on the\n",
    "    expected utility of doing a in state s, \n",
    "    according to T and u. This action is\n",
    "    the one that maximize the expected\n",
    "    utility.\n",
    "    @param u utility vector\n",
    "    @param T transition matrix\n",
    "    @param v starting vector\n",
    "    @return expected action (int)\n",
    "    \"\"\"\n",
    "    actions_array = np.zeros(4)\n",
    "    for action in range(4):\n",
    "        \n",
    "       #Expected utility of doing a in state s, according to T and u.\n",
    "       actions_array[action] = np.sum(np.multiply(u, np.dot(v, T[:,:,action])))\n",
    "        \n",
    "    return np.argmax(actions_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(p, shape):\n",
    "    \"\"\"Printing utility.\n",
    "\n",
    "    Print the policy actions using symbols:\n",
    "    ^, v, <, > up, down, left, right\n",
    "    * terminal states\n",
    "    # obstacles\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    policy_string = \"\"\n",
    "    for row in range(shape[0]):\n",
    "        for col in range(shape[1]):\n",
    "            if(p[counter] == -1): policy_string += \" *  \"            \n",
    "            elif(p[counter] == 0): policy_string += \" ^  \"\n",
    "            elif(p[counter] == 1): policy_string += \" <  \"\n",
    "            elif(p[counter] == 2): policy_string += \" v  \"           \n",
    "            elif(p[counter] == 3): policy_string += \" >  \"\n",
    "            elif(np.isnan(p[counter])): policy_string += \" #  \"\n",
    "            counter += 1\n",
    "        policy_string += '\\n'\n",
    "    return policy_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main(gamma, epsilon, p, r, tot_row, tot_col, T, method):\n",
    "    \n",
    "    ## Creating a set that will save all of values \n",
    "    lin_alg_set = set()\n",
    "    \n",
    "    ## Creates a blank utility vector\n",
    "    u = np.zeros(len(r))\n",
    "    \n",
    "    ## Keeps the iteration count\n",
    "    iteration = 0 \n",
    "                 \n",
    "    ## Creating a loop that will find the vest solutions\n",
    "    while True:\n",
    "        \n",
    "        ## Increments the number of iterations\n",
    "        iteration += 1\n",
    "        \n",
    "        ## Calcualtes the utility values\n",
    "        u_0 = u.copy()\n",
    "        \n",
    "        \n",
    "        ## Policy Iteration method\n",
    "        if method == 'policy_iteration':\n",
    "            \n",
    "            ## Calculates the utility value via policy iteration\n",
    "            u = evaluate_utility_value(p, u, r, T, gamma)\n",
    "            \n",
    "            \n",
    "            ## Stopping criteria\n",
    "            if np.max(abs((u - u_0))) < epsilon * (1 - gamma) / gamma: break   \n",
    "        \n",
    "        \n",
    "        ## Linear Programming method\n",
    "        if method == 'linear_program':\n",
    "            \n",
    "            ## Calculates the utility values with linear programming\n",
    "            u = evaluate_utility_value_LP(p, u, r, T, gamma)\n",
    "            \n",
    "            ## Stopping criteria\n",
    "            if iteration >= np.ceil(np.log(max(r) / epsilon) / np.log(1 / gamma)): break\n",
    "        \n",
    "         \n",
    "        \"\"\"\n",
    "            Creating a loop that will calculate the best action\n",
    "            for the current state of the grid\n",
    "        \"\"\"\n",
    "        for s in range(len(r)):\n",
    "            \n",
    "            if not np.isnan(p[s]) and not p[s] == -1:\n",
    "                \n",
    "                ## Initializes the value vector\n",
    "                v = np.zeros((1, len(r)))\n",
    "                \n",
    "                ## Sets current value on the grid \n",
    "                v[0,s] = 1.0\n",
    "                \n",
    "                # Policy improvement\n",
    "                a = return_expected_action(u, T, v)         \n",
    "                \n",
    "                ## Determines if the action is not the same\n",
    "                ## if not then stores the action \n",
    "                if a != p[s]:\n",
    "                    p[s] = a\n",
    "        \n",
    "        ## Prints out the current directions in the grid world\n",
    "        print(policy(p,(tot_row,tot_col)))\n",
    "          \n",
    "    print(\"=================== FINAL RESULT ==================\")\n",
    "    print(\"Iterations: \" + str(iteration))\n",
    "    print(\"Delta: \" + str(np.max(np.absolute((u - u_0)))))\n",
    "    print(\"Gamma: \" + str(gamma))\n",
    "    print(\"Epsilon: \" + str(epsilon))\n",
    "    print(\"===================================================\")\n",
    "    print(u[0:4])\n",
    "    print(u[4:8])\n",
    "    print(u[8:12])\n",
    "    print(\"===================================================\")\n",
    "    print(policy(p, shape = (tot_row,tot_col)))\n",
    "    print(\"===================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Gridworld2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters for our grid world exmaple\n",
    "\"\"\"\n",
    "\n",
    "## Parameters for the gridworld\n",
    "tot_row = 3\n",
    "tot_col = 4\n",
    "black_cells = [(1,1)]\n",
    "green_cells = [(0,3)]\n",
    "red_cells = [(1,3)]\n",
    "\n",
    "## Discount factor\n",
    "gamma = .999\n",
    "\n",
    "## tolerence level\n",
    "epsilon = 0.001\n",
    "\n",
    "## Generates a random policy vector\n",
    "p = gen_policy(tot_row, tot_col, black_cells, green_cells, red_cells)\n",
    "\n",
    "\n",
    "## Parameters for the rewards vector\n",
    "reward = 0.04\n",
    "green_value = 1.0\n",
    "red_value = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generates the MDP transition matrix\n",
    "T = create_MDP_Trans_Matrix(tot_row, tot_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creates the reward vector for the word\n",
    "r = rewards_vector(tot_row, tot_col\n",
    "                   ,reward, green_value, red_value\n",
    "                  ,black_cells, green_cells, red_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== FINAL RESULT ==================\n",
      "Iterations: 6905\n",
      "Delta: 0.0\n",
      "Gamma: 0.999\n",
      "Epsilon: 0.001\n",
      "===================================================\n",
      "[0.67893682 0.79738166 0.84842943 1.        ]\n",
      "[0.         0.         0.22042748 0.        ]\n",
      "[0.         0.         0.13616564 0.        ]\n",
      "===================================================\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   >   ^   <  \n",
      "\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "main(gamma, epsilon, p, r, tot_row, tot_col, T, 'linear_program')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration (Dynamic Programming Approach)\n",
    "\n",
    "Policy iteration is the traditional way to solve this problem by the following algorithm:\n",
    "\n",
    "1. $Policy \\ Evaluation$: Given a policy $\\pi_k$, evaluate $U^{\\pi_k}$\n",
    "2. $Policy \\ Improvement$: Compute the greedy policy:\n",
    "\n",
    "$$\\pi_{k+1}(s) = \\arg\\max_{a \\in A} \\{r\\ (s, a) + \\gamma {\\sum_{s' \\in \\ S}P(s_t = s'| s_t = s, a) \\ U^{\\pi}(s') \\} }$$\n",
    "\n",
    "The iterations will continue to until $U^{\\pi_{k+1}} = U^{\\pi_{k}}$\n",
    "\n",
    "\n",
    "Remark: Notice that we call $\\pi_{k+1}(s)$ greedy policy becuase it is maximizing the current estimate of the future rewards.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ^   <   >   *  \n",
      " ^   #   <   *  \n",
      " <   <   ^   <  \n",
      "\n",
      " ^   >   >   *  \n",
      " ^   #   ^   *  \n",
      " <   >   ^   <  \n",
      "\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " >   >   ^   <  \n",
      "\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   >   ^   <  \n",
      "\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   >   ^   <  \n",
      "\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   >   ^   <  \n",
      "\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   ^   <  \n",
      "\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "=================== FINAL RESULT ==================\n",
      "Iterations: 20\n",
      "Delta: 7.615595734589142e-07\n",
      "Gamma: 0.999\n",
      "Epsilon: 0.001\n",
      "===================================================\n",
      "[0.80796343 0.86539911 0.91653199 1.        ]\n",
      "[ 0.75696621  0.          0.65836281 -1.        ]\n",
      "[0.69968281 0.64882077 0.6047194  0.38150391]\n",
      "===================================================\n",
      " >   >   >   *  \n",
      " ^   #   ^   *  \n",
      " ^   <   <   <  \n",
      "\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "main(gamma, epsilon, p, r, tot_row, tot_col, T, 'policy_iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our results match! \n",
    "\n",
    "# Any questions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
